[
  {
    "path": "posts/2021-12-31-finetuning-codebert/",
    "title": "Finetuning CodeBERT for Semantic Code Search with PyTorch and Huggingface",
    "description": "Over the past few years, search engines have become more powerful thanks to the rise of deep learning methods allowing for more efficient search. Software engineering research has also greatly capitalized on this      re-emergence of deep learning to make software development more effective, for instance by making tasks such as code search more efficient than with traditional information retrieval techniques. In this post, we'll take a look on how to finetune CodeBERT,  a state-of-the-art model of code, for code search using PyTorch and HuggingFace, in less than 100 lines of code.",
    "author": [
      {
        "name": "Martin Weyssow",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-12-31",
    "categories": [
      "CodeBERT",
      "code search",
      "finetuning",
      "BERT"
    ],
    "contents": "\r\n\r\nContents\r\nWhat is semantic code search?\r\nFinetuning CodeBERT for code search\r\nDataset\r\nDependencies and environment setup\r\nModel and data loading\r\nTraining loop\r\nTesting the model\r\n\r\nResults analysis and discussion\r\nConclusion\r\n\r\nWhat is semantic code search?\r\nCode search consists of retrieving relevant code snippets from a corpus of code given a query. The query intends to bear the userâ€™s information need and is usually expressed as a natural language formulation. That is, the query encodes semantic meaning about what a user is seeking to implement, e.g., how to compute matrix multiplication. This is very similar to any search engine whose main purpose is to provide useful information to the user based on an input query.\r\nFor more details on semantic code search, I highly recommend reading the following blog post: https://github.blog/2018-09-18-towards-natural-language-semantic-code-search/\r\nThe remaining of this blog post focuses on the finetuning of CodeBERT for code search without going into much details about the architecture of the model which can be found in the paper: https://arxiv.org/abs/2002.08155\r\nFinetuning CodeBERT for code search\r\nDataset\r\nBefore diving into the implementation, let us choose a corpus of code that can be used for code search. Fortunately for us, we can use CodeSearchNet dataset which has been widely used in the code search research. We are going to use a sanitized version of the dataset published by the CodeBERT team.\r\nProgramming language\r\nTraining\r\nValid\r\nTest\r\nCandidates code\r\nPython\r\n251,820\r\n13,914\r\n14,918\r\n43,827\r\nPHP\r\n241,241\r\n12,982\r\n14,014\r\n52,660\r\nGo\r\n167,288\r\n7,325\r\n8,122\r\n28,120\r\nJava\r\n164,923\r\n5,183\r\n10,955\r\n40,347\r\nJavascript\r\n58,025\r\n3,885\r\n3,291\r\n13,981\r\nRuby\r\n24,927\r\n1,400\r\n1,261\r\n4,360\r\nThe dataset is available for 6 programming languages each of them divided into training/validation/test and candidates code. In this tutorial, we are going to use the Java dataset but the implementation described in this post can be used with the language of your choice.\r\nFor practical reasons, I uploaded all 6 datasets on Huggingface Hub. For Java, you can check it out here: https://huggingface.co/datasets/martiwey/codesearchnet-java\r\nFor the other ones, simply change java in the url by the desired programming language.\r\nDependencies and environment setup\r\nThe main libraries you need are the following: PyTorch, Transformers, Datasets, and Accelerate. Thanks to the latter, there is no need to setup your environment for GPU usage as the library will do it for you without having to change anything in your code.\r\nModel and data loading\r\nAs CodeBERT is already a pretrained model, there is no need to train a new tokenizer or the model from scratch. Everything can be loaded using the two following lines of code.\r\n\r\nfrom transformers import AutoTokenizer, AutoModel\r\ntokenizer = AutoTokenizer.from_pretrained('microsoft/codebert-base')\r\nmodel = AutoModel.from_pretrained('microsoft/codebert-base')\r\n\r\n\r\n\r\n\r\nThen, we simply load the CodeSearchNet java dataset. The datasets library will take care of caching everything for a faster access to the data.\r\n\r\nfrom datasets import load_dataset\r\ndataset_train = load_dataset('martiwey/codesearchnet-java', split='train')\r\ndataset_valid = load_dataset('martiwey/codesearchnet-java', split='validation')\r\ndataset_codebase = load_dataset('martiwey/codesearchnet-java', split='train', data_files='codebase.jsonl')\r\n\r\nTraining loop\r\nTesting the model\r\nResults analysis and discussion\r\nConclusion\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-31-finetuning-codebert/../../images/shared_vecs.png",
    "last_modified": "2022-01-04T14:44:55+01:00",
    "input_file": "finetuning-codebert.knit.md",
    "preview_width": 960,
    "preview_height": 540
  }
]
